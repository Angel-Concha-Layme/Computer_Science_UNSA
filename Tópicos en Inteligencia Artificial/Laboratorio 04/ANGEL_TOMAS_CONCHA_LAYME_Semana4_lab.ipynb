{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvxgIQgQv1r9"
      },
      "source": [
        "Dado el siguiente procedimiento para clasificar sentimientos usando el conjunto de datos imdb.\n",
        "1. Ejecute el procedimiento y compare el resultado de las variables accuracy_lstm y accuracy_cnn_lstm.\n",
        "2. Replique el procedimiento para el conjunto de datos enviado en anexo para crear un clasificador de sentimientos en espaniol (Big_AHR.csv.zip).\n",
        "3. Compare y muestre los resultados obtenidos usando solo LSTM y CNN + LSTM de sus clasificador en espaniol.\n",
        "\n",
        "\n",
        "(*) En caso de problema de ejecución por falta de recursos. puede crear  un subconjunto del archivo Big_AHR.csv.zip\n",
        "\n",
        "(*) Use los siguientes links como referencia.\n",
        "\n",
        "1. https://github.com/anandsarank/cnn-lstm-text-classification/blob/main/CNN%20with%20LSTM%20for%20Text%20Classification.ipynb\n",
        "2. https://colab.research.google.com/github/alvinntnu/python-notes/blob/master/nlp/sentiment-analysis-lstm-v1.ipynb\n",
        "3. https://www.kaggle.com/code/chizhikchi/lstm-binary-sentiment-classification-for-spanish/notebook\n",
        "4. https://www.kaggle.com/code/chizhikchi/ahr-corpus-presentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_4F8h8ak_J_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Conv1D,MaxPooling1D\n",
        "from tensorflow.keras.layers import LSTM,Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "np.random.seed(7)\n",
        "from prettytable import PrettyTable\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biGDDbBQqIuv",
        "outputId": "5a5a8cca-0851-4ee1-9237-6b924120870b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 1s 0us/step\n",
            "Shape of train data: (20000,)\n",
            "Shape of Test data: (25000,)\n",
            "Shape of CV data: (5000,)\n"
          ]
        }
      ],
      "source": [
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 10000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "X_train,X_cv,y_train,y_cv = train_test_split(X_train,y_train,test_size = 0.2)\n",
        "print(\"Shape of train data:\", X_train.shape)\n",
        "print(\"Shape of Test data:\", X_test.shape)\n",
        "print(\"Shape of CV data:\", X_cv.shape)\n",
        "\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 600\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "X_cv = sequence.pad_sequences(X_cv,maxlen=max_review_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H45oLVADCkKl"
      },
      "source": [
        "# Utilizando el dataset IMDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO_zlVcovi3W"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l8gdjnqqKt_",
        "outputId": "cf1ba8f5-6b1b-46dd-cadd-10bf8584ccc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 600, 32)           320000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               53200     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 373,301\n",
            "Trainable params: 373,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.6186 - accuracy: 0.6503\n",
            "Epoch 1: val_accuracy improved from -inf to 0.83200, saving model to weights_best.hdf5\n",
            "79/79 [==============================] - 354s 4s/step - loss: 0.6186 - accuracy: 0.6503 - val_loss: 0.4309 - val_accuracy: 0.8320\n",
            "Epoch 2/5\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.3362 - accuracy: 0.8597\n",
            "Epoch 2: val_accuracy improved from 0.83200 to 0.86080, saving model to weights_best.hdf5\n",
            "79/79 [==============================] - 349s 4s/step - loss: 0.3362 - accuracy: 0.8597 - val_loss: 0.3355 - val_accuracy: 0.8608\n",
            "Epoch 3/5\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2343 - accuracy: 0.9114\n",
            "Epoch 3: val_accuracy improved from 0.86080 to 0.88000, saving model to weights_best.hdf5\n",
            "79/79 [==============================] - 349s 4s/step - loss: 0.2343 - accuracy: 0.9114 - val_loss: 0.2945 - val_accuracy: 0.8800\n",
            "Epoch 4/5\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1781 - accuracy: 0.9372\n",
            "Epoch 4: val_accuracy did not improve from 0.88000\n",
            "79/79 [==============================] - 351s 4s/step - loss: 0.1781 - accuracy: 0.9372 - val_loss: 0.3705 - val_accuracy: 0.8738\n",
            "Epoch 5/5\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2209 - accuracy: 0.9177\n",
            "Epoch 5: val_accuracy did not improve from 0.88000\n",
            "79/79 [==============================] - 350s 4s/step - loss: 0.2209 - accuracy: 0.9177 - val_loss: 0.3277 - val_accuracy: 0.8756\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x79421788be20>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "filepath=\"weights_best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)\n",
        "callbacks_list = [checkpoint]\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=256,verbose = 1,callbacks = callbacks_list,validation_data=(X_cv,y_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHqnf1gaqQF7",
        "outputId": "5d7f0a73-9ba4-42ab-e0d4-74ce2582ead7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "98/98 [==============================] - 88s 891ms/step - loss: 0.3064 - accuracy: 0.8730\n",
            "Accuracy using LSTM: 87.30%\n"
          ]
        }
      ],
      "source": [
        "# Final evaluation of the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.load_weights(\"weights_best.hdf5\")\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "scores = model.evaluate(X_test, y_test, verbose=1,batch_size = 256)\n",
        "accuracy_lstm = scores[1]*100\n",
        "print(\"Accuracy using LSTM: %.2f%%\" % (accuracy_lstm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPDQRJJNvoY4"
      },
      "source": [
        "## CNN + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyVW0zGps-BD",
        "outputId": "b13a0beb-8015-46fd-be0c-a964c476cd98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 600, 32)           320000    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 600, 32)           3104      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 300, 32)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 100)               53200     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 376,405\n",
            "Trainable params: 376,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.6310 - accuracy: 0.6525\n",
            "Epoch 1: val_accuracy improved from -inf to 0.84560, saving model to weights_best_cnn.hdf5\n",
            "79/79 [==============================] - 107s 1s/step - loss: 0.6310 - accuracy: 0.6525 - val_loss: 0.3897 - val_accuracy: 0.8456\n",
            "Epoch 2/5\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2778 - accuracy: 0.8885\n",
            "Epoch 2: val_accuracy improved from 0.84560 to 0.86800, saving model to weights_best_cnn.hdf5\n",
            "79/79 [==============================] - 104s 1s/step - loss: 0.2778 - accuracy: 0.8885 - val_loss: 0.3097 - val_accuracy: 0.8680\n",
            "Epoch 3/5\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2178 - accuracy: 0.9207\n",
            "Epoch 3: val_accuracy did not improve from 0.86800\n",
            "79/79 [==============================] - 103s 1s/step - loss: 0.2178 - accuracy: 0.9207 - val_loss: 0.3697 - val_accuracy: 0.8592\n",
            "Epoch 4/5\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.3220 - accuracy: 0.8635\n",
            "Epoch 4: val_accuracy improved from 0.86800 to 0.87740, saving model to weights_best_cnn.hdf5\n",
            "79/79 [==============================] - 104s 1s/step - loss: 0.3220 - accuracy: 0.8635 - val_loss: 0.3279 - val_accuracy: 0.8774\n",
            "Epoch 5/5\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1463 - accuracy: 0.9470\n",
            "Epoch 5: val_accuracy did not improve from 0.87740\n",
            "79/79 [==============================] - 104s 1s/step - loss: 0.1463 - accuracy: 0.9470 - val_loss: 0.3378 - val_accuracy: 0.8740\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x79420f87a7d0>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "filepath=\"weights_best_cnn.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)\n",
        "callbacks_list = [checkpoint]\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=256,verbose = 1,callbacks = callbacks_list,validation_data=(X_cv,y_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXXP4sPrtWW-",
        "outputId": "b5cc23f2-56c4-43b9-8b87-50450a871c83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 600, 32)           320000    \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 600, 32)           3104      \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 300, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 100)               53200     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 376,405\n",
            "Trainable params: 376,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Accuracy CNN using LSTM: 87.32%\n"
          ]
        }
      ],
      "source": [
        "# Final evaluation of the model\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.load_weights(\"weights_best_cnn.hdf5\")\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "accuracy_cnn_lstm = scores[1]*100\n",
        "print(\"Accuracy CNN using LSTM: %.2f%%\" % (accuracy_cnn_lstm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48WchBnzCrnH"
      },
      "source": [
        "## Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5T9q-QqtpIQ",
        "outputId": "f8b59215-5482-4ca8-a443-dac7094d49c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+-------------------+\n",
            "|     Model      |      Accuracy     |\n",
            "+----------------+-------------------+\n",
            "|      LSTM      | 87.32399940490723 |\n",
            "| CNN using LSTM | 87.30400204658508 |\n",
            "+----------------+-------------------+\n"
          ]
        }
      ],
      "source": [
        "table = PrettyTable()\n",
        "table.field_names = ['Model', 'Accuracy']\n",
        "table.add_row(['LSTM', accuracy_cnn_lstm])\n",
        "table.add_row(['CNN using LSTM', accuracy_lstm])\n",
        "print(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ccb43H_SLiRi"
      },
      "source": [
        "Los resultados muestran que el modelo CNN+LSTM tuvo un tiempo de entrenamiento significativamente más rápido que el modelo LSTM.\n",
        "\n",
        "Esto sucede debido a que el modelo CNN+LSTM, gracias a la capa CNN permite al modelo aprender características espaciales y temporales de manera más eficiente, en este caso en concreto ayudan a capturar patrones locales y a reducir la dimensaionalidad del texto.  \n",
        "\n",
        "Con respecto a la precisión, ambos modelos son igual de eficientes, pero dado que el modelo CNN+LSTM logró obtener resultados comparables con una fracción del tiempo de entrenamiento requerido por el modelo LSTM, es evidente que el enfoque CNN+LSTM es más eficiente en términos de tiempo de entrenamiento en este caso en particular"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8D5akF1B2QM"
      },
      "source": [
        "# Utilizando el dataset BIG_AHR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BAjX0iM8FqK"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dluGPcvT72ik",
        "outputId": "cfd66985-5bc5-4297-aadc-f2169e5dba85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_layer (Embedding)  (None, 600, 32)          320000    \n",
            "                                                                 \n",
            " lstm_layer (LSTM)           (None, 100)               53200     \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 373,301\n",
            "Trainable params: 373,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "36/36 [==============================] - ETA: 0s - loss: -0.2984 - accuracy: 0.7097\n",
            "Epoch 1: val_accuracy improved from -inf to 0.71768, saving model to weights_best.hdf5\n",
            "36/36 [==============================] - 162s 4s/step - loss: -0.2984 - accuracy: 0.7097 - val_loss: -1.1201 - val_accuracy: 0.7177\n",
            "Epoch 2/5\n",
            "36/36 [==============================] - ETA: 0s - loss: -1.4634 - accuracy: 0.7190\n",
            "Epoch 2: val_accuracy did not improve from 0.71768\n",
            "36/36 [==============================] - 162s 4s/step - loss: -1.4634 - accuracy: 0.7190 - val_loss: -1.6377 - val_accuracy: 0.7177\n",
            "Epoch 3/5\n",
            "36/36 [==============================] - ETA: 0s - loss: -1.8934 - accuracy: 0.7190\n",
            "Epoch 3: val_accuracy did not improve from 0.71768\n",
            "36/36 [==============================] - 162s 5s/step - loss: -1.8934 - accuracy: 0.7190 - val_loss: -2.0170 - val_accuracy: 0.7177\n",
            "Epoch 4/5\n",
            "36/36 [==============================] - ETA: 0s - loss: -2.2649 - accuracy: 0.7190\n",
            "Epoch 4: val_accuracy did not improve from 0.71768\n",
            "36/36 [==============================] - 161s 4s/step - loss: -2.2649 - accuracy: 0.7190 - val_loss: -2.3422 - val_accuracy: 0.7177\n",
            "Epoch 5/5\n",
            "36/36 [==============================] - ETA: 0s - loss: -2.6104 - accuracy: 0.7190\n",
            "Epoch 5: val_accuracy did not improve from 0.71768\n",
            "36/36 [==============================] - 162s 5s/step - loss: -2.6104 - accuracy: 0.7190 - val_loss: -2.6792 - val_accuracy: 0.7177\n",
            "The model took 865.6520409584045 seconds to train.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Define hyperparameters\n",
        "embedding_vector_length = 32\n",
        "lstm_units = 100\n",
        "dropout_rate = 0.2\n",
        "num_epochs = 5\n",
        "batch_size = 256\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=top_words,\n",
        "                    output_dim=embedding_vector_length,\n",
        "                    input_length=max_review_length,\n",
        "                    name=\"embedding_layer\"))\n",
        "model.add(LSTM(units=lstm_units,\n",
        "               dropout=dropout_rate,\n",
        "               recurrent_dropout=dropout_rate,\n",
        "               name=\"lstm_layer\"))\n",
        "model.add(Dense(units=1, activation='sigmoid', name=\"output_layer\"))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Specify the path for model weights\n",
        "weights_filepath=\"weights_best.hdf5\"\n",
        "\n",
        "# Set callbacks\n",
        "checkpoint = ModelCheckpoint(weights_filepath,\n",
        "                             monitor='val_accuracy',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='max',\n",
        "                             save_weights_only=True)\n",
        "early_stop = EarlyStopping(monitor='val_accuracy',\n",
        "                           patience=5,\n",
        "                           restore_best_weights=True)\n",
        "\n",
        "callbacks_list = [checkpoint, early_stop]\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train,\n",
        "          y_train,\n",
        "          epochs=num_epochs,\n",
        "          batch_size=batch_size,\n",
        "          verbose=1,\n",
        "          callbacks=callbacks_list,\n",
        "          validation_data=(X_cv, y_cv))\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the time taken to train the model\n",
        "training_time = end_time - start_time\n",
        "print(f'The model took {training_time} seconds to train.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kynvebwg_drU",
        "outputId": "495d151f-4ae7-4034-a098-cb54025923f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 11s 863ms/step - loss: -1.2322 - accuracy: 0.7207\n",
            "Accuracy using LSTM: 72.07%\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Define hyperparameters\n",
        "embedding_vector_length = 32\n",
        "lstm_units = 100\n",
        "dropout_rate = 0.2\n",
        "batch_size = 256\n",
        "\n",
        "# Load model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=top_words,\n",
        "                    output_dim=embedding_vector_length,\n",
        "                    input_length=max_review_length,\n",
        "                    name=\"embedding_layer\"))\n",
        "model.add(LSTM(units=lstm_units,\n",
        "               dropout=dropout_rate,\n",
        "               recurrent_dropout=dropout_rate,\n",
        "               name=\"lstm_layer\"))\n",
        "model.add(Dense(units=1, activation='sigmoid', name=\"output_layer\"))\n",
        "\n",
        "# Load the best weights\n",
        "weights_filepath = \"weights_best.hdf5\"\n",
        "model.load_weights(weights_filepath)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Evaluate the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=1, batch_size=batch_size)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy_lstm = scores[1] * 100\n",
        "print(f\"Accuracy using LSTM: {accuracy_lstm:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6buvBV3_kDu"
      },
      "source": [
        "## CNN + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4dVi9Ig_wbT",
        "outputId": "806886c3-b815-4961-9e93-76126f48056f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_layer (Embedding)  (None, 600, 32)          320000    \n",
            "                                                                 \n",
            " conv1d_layer (Conv1D)       (None, 600, 32)           3104      \n",
            "                                                                 \n",
            " maxpooling1d_layer (MaxPool  (None, 300, 32)          0         \n",
            " ing1D)                                                          \n",
            "                                                                 \n",
            " lstm_layer (LSTM)           (None, 100)               53200     \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 376,405\n",
            "Trainable params: 376,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "36/36 [==============================] - ETA: 0s - loss: -0.4691 - accuracy: 0.7161\n",
            "Epoch 1: val_accuracy improved from -inf to 0.71768, saving model to weights_best_cnn.hdf5\n",
            "36/36 [==============================] - 52s 1s/step - loss: -0.4691 - accuracy: 0.7161 - val_loss: -1.1420 - val_accuracy: 0.7177\n",
            "Epoch 2/5\n",
            "36/36 [==============================] - ETA: 0s - loss: -1.4651 - accuracy: 0.7190\n",
            "Epoch 2: val_accuracy did not improve from 0.71768\n",
            "36/36 [==============================] - 47s 1s/step - loss: -1.4651 - accuracy: 0.7190 - val_loss: -1.6276 - val_accuracy: 0.7177\n",
            "Epoch 3/5\n",
            "36/36 [==============================] - ETA: 0s - loss: -1.8829 - accuracy: 0.7190\n",
            "Epoch 3: val_accuracy did not improve from 0.71768\n",
            "36/36 [==============================] - 45s 1s/step - loss: -1.8829 - accuracy: 0.7190 - val_loss: -1.9973 - val_accuracy: 0.7177\n",
            "Epoch 4/5\n",
            "36/36 [==============================] - ETA: 0s - loss: -2.2476 - accuracy: 0.7190\n",
            "Epoch 4: val_accuracy did not improve from 0.71768\n",
            "36/36 [==============================] - 48s 1s/step - loss: -2.2476 - accuracy: 0.7190 - val_loss: -2.3323 - val_accuracy: 0.7177\n",
            "Epoch 5/5\n",
            "36/36 [==============================] - ETA: 0s - loss: -2.5978 - accuracy: 0.7190\n",
            "Epoch 5: val_accuracy did not improve from 0.71768\n",
            "36/36 [==============================] - 49s 1s/step - loss: -2.5978 - accuracy: 0.7190 - val_loss: -2.6557 - val_accuracy: 0.7177\n",
            "The model took 264.4450755119324 seconds to train.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define hyperparameters\n",
        "embedding_vector_length = 32\n",
        "conv1d_filters = 32\n",
        "conv1d_kernel_size = 3\n",
        "pool_size = 2\n",
        "lstm_units = 100\n",
        "num_epochs = 5\n",
        "batch_size = 256\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=top_words,\n",
        "                    output_dim=embedding_vector_length,\n",
        "                    input_length=max_review_length,\n",
        "                    name=\"embedding_layer\"))\n",
        "model.add(Conv1D(filters=conv1d_filters,\n",
        "                 kernel_size=conv1d_kernel_size,\n",
        "                 padding='same',\n",
        "                 activation='relu',\n",
        "                 name=\"conv1d_layer\"))\n",
        "model.add(MaxPooling1D(pool_size=pool_size, name=\"maxpooling1d_layer\"))\n",
        "model.add(LSTM(units=lstm_units, name=\"lstm_layer\"))\n",
        "model.add(Dense(units=1, activation='sigmoid', name=\"output_layer\"))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Specify the path for model weights\n",
        "weights_filepath=\"weights_best_cnn.hdf5\"\n",
        "\n",
        "# Set callbacks\n",
        "checkpoint = ModelCheckpoint(weights_filepath,\n",
        "                             monitor='val_accuracy',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='max',\n",
        "                             save_weights_only=True)\n",
        "\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train,\n",
        "          y_train,\n",
        "          epochs=num_epochs,\n",
        "          batch_size=batch_size,\n",
        "          verbose=1,\n",
        "          callbacks=callbacks_list,\n",
        "          validation_data=(X_cv, y_cv))\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the time taken to train the model\n",
        "training_time = end_time - start_time\n",
        "print(f'The model took {training_time} seconds to train.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRWyNUuvBbL7",
        "outputId": "8e8e6fa6-c7b8-40fc-e653-31557fe37484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_layer (Embedding)  (None, 600, 32)          320000    \n",
            "                                                                 \n",
            " conv1d_layer (Conv1D)       (None, 600, 32)           3104      \n",
            "                                                                 \n",
            " maxpooling1d_layer (MaxPool  (None, 300, 32)          0         \n",
            " ing1D)                                                          \n",
            "                                                                 \n",
            " lstm_layer (LSTM)           (None, 100)               53200     \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 376,405\n",
            "Trainable params: 376,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Accuracy CNN using LSTM: 72.07%\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense\n",
        "\n",
        "# Define hyperparameters\n",
        "embedding_vector_length = 32\n",
        "conv1d_filters = 32\n",
        "conv1d_kernel_size = 3\n",
        "pool_size = 2\n",
        "lstm_units = 100\n",
        "batch_size = 256\n",
        "\n",
        "# Load model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=top_words,\n",
        "                    output_dim=embedding_vector_length,\n",
        "                    input_length=max_review_length,\n",
        "                    name=\"embedding_layer\"))\n",
        "model.add(Conv1D(filters=conv1d_filters,\n",
        "                 kernel_size=conv1d_kernel_size,\n",
        "                 padding='same',\n",
        "                 activation='relu',\n",
        "                 name=\"conv1d_layer\"))\n",
        "model.add(MaxPooling1D(pool_size=pool_size, name=\"maxpooling1d_layer\"))\n",
        "model.add(LSTM(units=lstm_units, name=\"lstm_layer\"))\n",
        "model.add(Dense(units=1, activation='sigmoid', name=\"output_layer\"))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Load the best weights\n",
        "weights_filepath = \"weights_best_cnn.hdf5\"\n",
        "model.load_weights(weights_filepath)\n",
        "\n",
        "# Evaluate the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy_cnn_lstm = scores[1] * 100\n",
        "print(f\"Accuracy CNN using LSTM: {accuracy_cnn_lstm:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11Oi8_BoCbqO"
      },
      "source": [
        "## Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz_YcagjBluk",
        "outputId": "434c0d98-070b-49bf-8e66-c18eee3d36dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+-------------------+\n",
            "|     Model      |      Accuracy     |\n",
            "+----------------+-------------------+\n",
            "|      LSTM      | 72.07175493240356 |\n",
            "| CNN using LSTM | 72.07175493240356 |\n",
            "+----------------+-------------------+\n"
          ]
        }
      ],
      "source": [
        "table = PrettyTable()\n",
        "table.field_names = ['Model', 'Accuracy']\n",
        "table.add_row(['LSTM', accuracy_lstm])\n",
        "table.add_row(['CNN using LSTM', accuracy_cnn_lstm])\n",
        "print(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RntH0OIvJWS7"
      },
      "source": [
        "Los resultados muestran que el modelo CNN+LSTM tuvo un tiempo de entrenamiento significativamente más rápido que el modelo LSTM. El tiempo de entrenamiento del modelo CNN+LSTM fue de aproximadamente 264 segundos, mientras que el modelo LSTM tomó alrededor de 886 segundos (los tiempos se calcularon al momento de entrenar el modelo, revisar las secciones correspondientes).\n",
        "\n",
        "Esto sucede debido a que el modelo CNN+LSTM, gracias a la capa CNN permite al modelo aprender características espaciales y temporales de manera más eficiente, en este caso en concreto ayudan a capturar patrones locales y a reducir la dimensaionalidad del texto.  \n",
        "\n",
        "Con respecto a la precisión, ambos modelos son igual de eficientes, pero dado que el modelo CNN+LSTM logró obtener resultados comparables con una fracción del tiempo de entrenamiento requerido por el modelo LSTM, es evidente que el enfoque CNN+LSTM es más eficiente en términos de tiempo de entrenamiento en este caso en particular. (sucede exactamente lo mismo para el primer ejemplo, con el conjunto de datos en ingles)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
